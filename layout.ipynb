{
 "cells": [
  {
   "cell_type": "code",
   "id": "466aa3af3ba0ac79",
   "metadata": {},
   "source": [
    "!pip install paddlepaddle==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\n",
    "!pip install paddleocr==3.0.1\n",
    "!pip install pandas\n",
    "!pip show paddleocr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24c579a4bea3742",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "from src.helpers import show\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33f541aa3dd61ab4",
   "metadata": {},
   "source": [
    "# Load the image\n",
    "pages = convert_from_path(\"./testdaten/001.pdf\")\n",
    "page = pages[0]\n",
    "\n",
    "image = cv2.cvtColor(np.array(pages[0]), cv2.COLOR_RGB2BGR)\n",
    "image = cv2.resize(image, (1654, 2338))\n",
    "\n",
    "image_height = image.shape[0]\n",
    "image_width = image.shape[1]\n",
    "image_area = image_height * image_width\n",
    "\n",
    "print(f\"Height: {image_height}, Width: {image_width}, Area: {image_area}\")\n",
    "\n",
    "show(image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87f3d0cb741c8598",
   "metadata": {},
   "source": [
    "# Document Scan"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4be230c67216e30",
   "metadata": {},
   "source": [
    "from src.scanning import scan_image\n",
    "\n",
    "image = scan_image(image)\n",
    "show(image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e91dd530606d6551",
   "metadata": {},
   "source": [
    "# Layout Parsing\n",
    "\n",
    "In diesem Abschnitt werden die BoundingRects der einzelnen auszufüllenden Zeilen bestimmt. Dafür werden folgende Schritte durchgeführt:\n",
    "\n",
    "- Section Konturen bestimmen\n",
    "- Field Konturen bestimmen\n",
    "\n",
    "## Section Konturen bestimmen\n",
    "\n",
    "Erkennung der 7 größten Konturen im Bild, nach Koordinaten sortieren und den Sections zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "id": "14cb3ffda9998d75",
   "metadata": {},
   "source": [
    "gray = cv2.medianBlur(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), 3)\n",
    "show(gray)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "582bd54c166b64d4",
   "metadata": {},
   "source": [
    "# Thresholding the image\n",
    "img_bin = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 5)\n",
    "show(img_bin)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "131ff3d4ae780079",
   "metadata": {},
   "source": [
    "edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "# Apply dilation to make edges thicker# Define a kernel\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
    "\n",
    "# Dilation and Erosion\n",
    "edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "edges = cv2.erode(edges, kernel, iterations=1)\n",
    "\n",
    "show(edges, figsize=(21, 12))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df796dceaad7dddf",
   "metadata": {},
   "source": [
    "# TODO: Eventuell corner detection noch ansehen\n",
    "\n",
    "# Find contours in the binary image\n",
    "contours, hierarchy = cv2.findContours(~img_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw rectangles around each contour\n",
    "img_sections = image.copy()\n",
    "target_rois = []\n",
    "\n",
    "rects = map(cv2.boundingRect, contours)\n",
    "rects = [(x, y, w, h) for (x, y, w, h) in rects if not (w / image_width < 0.3 or w / image_width > 0.5)]\n",
    "rects = sorted(rects, key=lambda x: x[0])\n",
    "\n",
    "rects_left = sorted(rects[:3], key=lambda x: x[1])\n",
    "rects_right = sorted(rects[3:], key=lambda x: x[1])\n",
    "\n",
    "section_bounds = pd.DataFrame({\n",
    "    \"Verstorbenendaten\": rects_left[0],\n",
    "    \"Ehepartner\": rects_left[1],\n",
    "    \"Auftraggeberdaten\": rects_left[2],\n",
    "    \"Bestattung\": rects_right[0],\n",
    "    \"Termine\": rects_right[1],\n",
    "    \"Zahlungsdaten\": rects_right[2],\n",
    "    \"Erhaltene Dokumente\": rects_right[3]\n",
    "})\n",
    "section_bounds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3536bfb3fc4c612",
   "metadata": {},
   "source": [
    "## Field Konturen bestimmen\n",
    "\n",
    "Anzahl der Zeilen (n) jeder Section durch Verwendung von `data/sections.csv` bestimmen, da jede Zeile die gleiche Höhe hat, kann die Section in n Zeilen mit gleicher Höhe aufgeteilt werden. Die Label für jede Zeile werden auch aus der CSV-Datei entnommen."
   ]
  },
  {
   "cell_type": "code",
   "id": "4ec401949205ac86",
   "metadata": {},
   "source": [
    "for key, (x, y, w, h) in section_bounds.items():\n",
    "    cv2.rectangle(img_sections, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(img_sections, key, (x, y), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 200), 2)\n",
    "\n",
    "show(img_sections)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8da8ca25883ca9d4",
   "metadata": {},
   "source": [
    "sections = pd.read_csv('data/sections.csv', index_col=[\"section_index\", \"field_index\"])\n",
    "img_fields = img_sections.copy()\n",
    "\n",
    "def get_points(row):\n",
    "    section_name = row[\"section\"]\n",
    "    field_index = row.name[1]\n",
    "    (x, y, w, h) = section_bounds[section_name]\n",
    "\n",
    "    parts = len(sections[sections.section == section_name])\n",
    "    part_size = h / parts\n",
    "    part_start_y = part_size * field_index\n",
    "\n",
    "    # Returns top-left and bottom-right coordinates of a rectangle in relation to the section\n",
    "    # point1: (x, part_start_y) - top left corner of the section\n",
    "    # point2: (x + width, part_start_y + part_size) - bottom right corner of the section\n",
    "    return [\n",
    "        np.array([0, part_start_y]).astype(np.int32),\n",
    "        np.array([w, part_start_y + part_size]).astype(np.int32)\n",
    "    ]\n",
    "\n",
    "sections[\"bounding_rect\"] = sections.apply(get_points, axis=1)\n",
    "\n",
    "for _, row in sections.iterrows():\n",
    "    field_name = row[\"field\"]\n",
    "    section_name = row[\"section\"]\n",
    "    (x, y, w, h) = section_bounds[section_name]\n",
    "    section_start = [x, y]\n",
    "\n",
    "    [point1, point2] = row[\"bounding_rect\"]\n",
    "\n",
    "    cv2.rectangle(\n",
    "        img_fields,\n",
    "        point1 + section_start,\n",
    "        point2 + section_start,\n",
    "        (0, 255, 0),\n",
    "        2)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    font_size = 0.5\n",
    "    font_thickness = 1\n",
    "    ([text_width, _], text_height) = cv2.getTextSize(field_name, font, font_size, font_thickness)\n",
    "    text_size = [text_width, text_height]\n",
    "    cv2.putText(img_fields, field_name, np.array(point2 - text_size + section_start).astype(np.int32), font, font_size, (255, 0, 200), font_thickness)\n",
    "\n",
    "show(img_fields)\n",
    "sections"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "780d5c5220247f1d",
   "metadata": {},
   "source": [
    "# Crop Sections"
   ]
  },
  {
   "cell_type": "code",
   "id": "31b78ec9bd86fd79",
   "metadata": {},
   "source": [
    "def cropToSection(section: str):\n",
    "    x, y, w, h = section_bounds[section]\n",
    "\n",
    "    result_img = image.copy()\n",
    "    cv2.rectangle(result_img, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "\n",
    "    # Crop and display the Verstorbenendaten section\n",
    "    cropped_section = result_img[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "    # Save the cropped section to a file\n",
    "    output_path = f'output/cropped/cropped_{section}_section.jpg'\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(cropped_section, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Saved cropped section to {output_path}\")\n",
    "    show(cropped_section, figsize=(3, 3), title=f'Cropped {section} section')\n",
    "\n",
    "    return cropped_section"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ce7ef3638713408",
   "metadata": {},
   "source": [
    "# Croppe Verstorbenendaten section and save it to a file\n",
    "cropped_sections = []\n",
    "for i, row in enumerate(section_bounds):\n",
    "    cropped_sections.append((row, cropToSection(row)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f57b89240e65a680",
   "metadata": {},
   "source": [
    "# Text Detection\n",
    "## Remove/Keep Blue Color Pixels"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b9eaa2c0cca4aa0",
   "metadata": {},
   "source": [
    "# Remove font that has been identified as black from the image\n",
    "# blue font should represent the handwritten text\n",
    "\n",
    "def only_color_between(img_bgr, lower_bound, upper_bound):\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create mask for blue pixels\n",
    "    blue_mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Create result image with white background\n",
    "    result = np.full_like(img_bgr, 255)\n",
    "\n",
    "    # Copy only blue pixels to result image\n",
    "    result = cv2.bitwise_and(img_bgr, img_bgr, mask=blue_mask)\n",
    "\n",
    "    # Set non-blue pixels to white\n",
    "    result[blue_mask == 0] = [255, 255, 255]\n",
    "    return result\n",
    "\n",
    "def remove_color_between(img_bgr, lower_bound, upper_bound):\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create mask for blue pixels\n",
    "    blue_mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Invert the mask: now non-blue areas are 255, blue areas are 0\n",
    "    non_blue_mask = cv2.bitwise_not(blue_mask)\n",
    "\n",
    "    # Create result image with white background\n",
    "    result = np.full_like(img_bgr, 255)\n",
    "\n",
    "    # Copy only blue pixels to result image\n",
    "    result = cv2.bitwise_and(img_bgr, img_bgr, mask=non_blue_mask)\n",
    "\n",
    "    # Set non-blue pixels to white\n",
    "    result[non_blue_mask == 0] = [255, 255, 255]\n",
    "    return result\n",
    "\n",
    "\n",
    "def only_blue_pixels(image_bgr):\n",
    "    lower_blue = np.array([90, 50, 50])\n",
    "    upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "    return only_color_between(image_bgr, lower_blue, upper_blue)\n",
    "\n",
    "def remove_blue_pixels(image_bgr):\n",
    "    lower_blue = np.array([90, 50, 50])\n",
    "    upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "    return remove_color_between(image_bgr, lower_blue, upper_blue)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f00282ef8d41203d",
   "metadata": {},
   "source": [
    "imgs_handwritten = []\n",
    "\n",
    "for i, (section, img) in enumerate(cropped_sections):\n",
    "    img_handwritten = only_blue_pixels(img)\n",
    "    imgs_handwritten.append((section, img_handwritten))\n",
    "\n",
    "    show(img_handwritten, figsize=(3, 3))\n",
    "    cv2.imwrite(f'output/handwritten/img_handwritten_{section}.jpg', img_handwritten)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d12a9ed921ccf453",
   "metadata": {},
   "source": "## Detect Text"
  },
  {
   "cell_type": "code",
   "id": "4167a024773ae985",
   "metadata": {},
   "source": [
    "from paddleocr import TextDetection\n",
    "\n",
    "model = TextDetection(model_name=\"PP-OCRv5_server_det\", enable_mkldnn=False)\n",
    "\n",
    "def detect_boxes(img_path, filename):\n",
    "    result = model.predict(img_path)\n",
    "    for row in result:\n",
    "        row.save_to_img(f\"output/detected/{filename}.jpg\")\n",
    "        \n",
    "        return row"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbf9503bfc97bc1d",
   "metadata": {},
   "source": [
    "hw_coordinates = {}\n",
    "hw_centers = {}\n",
    "\n",
    "# detect handwritten text with PaddleOCR\n",
    "for i, (section, img) in enumerate(imgs_handwritten):\n",
    "    img_path = f\"output/handwritten/img_handwritten_{section}.jpg\"\n",
    "    print(f\"Detect handwriting in section {section}...\")\n",
    "    # handwritten_boxes = detect_boxes(img_path, f\"detected_hw_boxes_{section}\")\n",
    "    result = detect_boxes(img_path, f\"detected_hw_boxes_{section}\")\n",
    "\n",
    "    boxes = result[\"dt_polys\"]\n",
    "    y_middles = []\n",
    "    for point in boxes:\n",
    "        y_coords = [p[1] for p in point]\n",
    "        min_y = min(y_coords)\n",
    "        max_y = max(y_coords)\n",
    "        y_middle = (min_y + max_y) / 2\n",
    "\n",
    "        y_middles.append(y_middle)\n",
    "\n",
    "    hw_centers[section] = y_middles\n",
    "    hw_coordinates[section] = boxes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7d710541eaba265",
   "metadata": {},
   "source": [
    "# assign handwritten boxes to label rows\n",
    "def assign_boxes_to_rows(hw_center, hw_coords, label_rows):\n",
    "    assignments = []\n",
    "\n",
    "    # Calculate center points for each handwritten box\n",
    "    for j, y in enumerate(hw_center):\n",
    "        # Check which row contains this center point\n",
    "        for i, row in enumerate(label_rows):\n",
    "            left_top_pt = row[0]\n",
    "            bottom_right_pt = row[1]\n",
    "\n",
    "            if left_top_pt[1] <= y <= bottom_right_pt[1]:\n",
    "                assignments.append((i, hw_coords[j]))\n",
    "\n",
    "    return assignments\n",
    "\n",
    "label_hw_assigments = {}\n",
    "\n",
    "# Create a dictionary mapping section indices to section names\n",
    "section_map = {idx: name for idx, name in enumerate(sections[\"section\"].unique())}\n",
    "\n",
    "for idx, name in section_map.items():\n",
    "    hw_section_center = hw_centers[name]\n",
    "    hw_section_coords = hw_coordinates[name]\n",
    "\n",
    "    section_label_boxes = sections[\"bounding_rect\"][idx]\n",
    "    label_hw_assigments[name] = assign_boxes_to_rows(hw_section_center, hw_section_coords, section_label_boxes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bc87ee81ccb4444",
   "metadata": {},
   "source": [
    "# save assignments to sections dataframe\n",
    "def get_hw_points(row):\n",
    "    field_index = row.name[1]\n",
    "    section_name = row[\"section\"]\n",
    "\n",
    "    hw_rects = []\n",
    "\n",
    "    section_assignments = label_hw_assigments[section_name]\n",
    "\n",
    "    for item in section_assignments:\n",
    "        if item[0] == field_index:\n",
    "            hw_rects.append(item[1])\n",
    "\n",
    "    return hw_rects\n",
    "\n",
    "sections[\"handwritten_bounding_rect\"] = sections.apply(get_hw_points, axis=1)\n",
    "sections"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24decff92a18b6c",
   "metadata": {},
   "source": [
    "# Text Recognition\n",
    "In diesem Abschnitt erfolgt die Erkennung der handschriftlichen Wörter oder Wortgruppen.\\\n",
    "Das Modell und der benötigte Przessor werden erst initialisiert.\n",
    "Dann werden die in den vorherigen Abschnitten ermittelten Textschnippsel genutzt und mit dem Transformer-basierten OCR-Modell \"TrOCR\" (https://doi.org/10.48550/arXiv.2109.10282) erkannt.\\\n",
    "Die erkannten Wörter werden anschließend für jede Zeile an die Datenstruktur angehängt, sobald die Erkennung abgeschlossen ist."
   ]
  },
  {
   "cell_type": "code",
   "id": "8702696590603e00",
   "metadata": {},
   "source": [
    "!pip install --upgrade tokenizers\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('fhswf/TrOCR_german_handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('/checkpoint-300')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f254b181356e756",
   "metadata": {},
   "source": [
    "def recognizeTROCR(image):\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "section_name = sections['section']\n",
    "field_name = sections[\"field\"]\n",
    "hw = sections['handwritten_bounding_rect']\n",
    "recognized_text = []\n",
    "stopper = 0 # Zum testen für schwächere Rechner oder zum Demonstrieren. Unterbricht die Auswertung wenn es einen bestimmten Wert erreicht.\n",
    "stopper_stop = 5 # wenn = -1, dann kein stoppen\n",
    "\n",
    "for g_index, _ in enumerate(sections.groupby('section')):\n",
    "    img = imgs_handwritten[g_index][1]\n",
    "\n",
    "    print(\"Schrifterkennung für Abschnitt \"+section_name.get(g_index)[0]+\" läuft...\")\n",
    "    for i, row in enumerate(hw.get(g_index)):\n",
    "        if(stopper_stop > -1 and stopper >= stopper_stop):\n",
    "            break\n",
    "\n",
    "        if len(row) > 0:\n",
    "            p1 = row[0][0]\n",
    "            p2 = row[0][1]\n",
    "            p3 = row[0][2]\n",
    "            p4 = row[0][3]\n",
    "\n",
    "            top_left_x = min([p1[0],p2[0],p3[0],p4[0]])\n",
    "            top_left_y = min([p1[1],p2[1],p3[1],p4[1]])\n",
    "            bot_right_x = max([p1[0],p2[0],p3[0],p4[0]])\n",
    "            bot_right_y = max([p1[1],p2[1],p3[1],p4[1]])\n",
    "\n",
    "            crop = img[top_left_y:bot_right_y, top_left_x:bot_right_x]\n",
    "\n",
    "            text = recognizeTROCR(crop)\n",
    "            recognized_text.append(text)\n",
    "\n",
    "            show(crop) # demonstrativ\n",
    "        else:\n",
    "            text = \"handwritten_bounding_rect ist leer. Nichts erkannt\"\n",
    "            recognized_text.append(text)\n",
    "\n",
    "        if (stopper_stop > -1):\n",
    "            stopper +=1\n",
    "\n",
    "        print(text) # demonstrativ\n",
    "\n",
    "    # Auffüllen des arrays damit es trotz Unvollständigkeit an die Datenstruktur angehangen werden kann\n",
    "    if(stopper_stop > -1):\n",
    "        while len(recognized_text) < len(sections):\n",
    "            recognized_text.append(\"Platzhalter\")\n",
    "\n",
    "sections[\"recognized_text\"] = recognized_text\n",
    "sections"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6bcc566ec89504a1",
   "metadata": {},
   "source": [
    "# Checkboxes\n",
    "\n",
    "In diesem Abschnitt werden die Checkboxen der einzelnen Zeilen erkannt und erkannt ob diese angekreuzt sind oder nicht. Dies passiert in 2 Schritten:\n",
    "\n",
    "## 1. Schritt - Positionen der Checkboxen bestimmen\n",
    "\n",
    "Hierfür wurde die Datei `data/choices.csv` angelegt in denen Labels für die einzelnen Checkboxen der jeweiligen Zeilen angegeben wurden. Die Reihenfolge dafür ist identisch zu der Reihenfolge in der diese im Formular auch auftauchen.\n",
    "\n",
    "Zur Bestimmung der Positionen der Checkboxen wird für jede Zeile einzeln die Funktion `findContours` von OpenCV benutzt. Dabei werden die n größten Konturen der Zeile verwendet, wobei n die Anzahl der Checkboxen in der aktuellen Zeile ist.\n",
    "\n",
    "## 2. Schritt - Herausfinden ob Checkbox angekreuzt\n",
    "\n",
    "Um herauszufinden ob die Checkbox angekreuzt wurde, werden die blauen Pixel in dem Bereich der Checkbox gezählt. Wenn dieser Wert einen bestimmten Wert überschreitet wird die Checkbox als angekreuzt erkannt."
   ]
  },
  {
   "cell_type": "code",
   "id": "22cb0440a6087ac3",
   "metadata": {},
   "source": [
    "row = sections.loc[0,10]\n",
    "\n",
    "def get_checkbox_images(row, field_labels):\n",
    "    count = len(field_labels)\n",
    "    (x, y, w, h) = section_bounds[row[\"section\"]]\n",
    "    [min, max] = row[\"bounding_rect\"]\n",
    "\n",
    "    # Removing blue pixels would leave us open contours therefore contour detection would not work correctly\n",
    "    # row_img = remove_blue_pixels(image[y+min[1]:y+max[1], x+min[0]:x+max[0]].copy())\n",
    "    row_img = image[y+min[1]:y+max[1], x+min[0]:x+max[0]].copy()\n",
    "    show(row_img, figsize=(4, 1))\n",
    "\n",
    "    row_blur = cv2.pyrMeanShiftFiltering(row_img, 16, 64)\n",
    "    #show(row_blur)\n",
    "    row_gray = cv2.cvtColor(row_blur, cv2.COLOR_BGR2GRAY)\n",
    "    #show(row_gray)\n",
    "    row_bin = cv2.threshold(row_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    #show(row_bin)\n",
    "\n",
    "    row_cnts, row_hierarchy = cv2.findContours(row_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Keeping only the largest detected contour\n",
    "    row_page = sorted(row_cnts, key=cv2.contourArea, reverse=True)[:count]\n",
    "    row_rects = sorted([*map(cv2.boundingRect, row_page)], key=lambda x: x[0])\n",
    "    row_checkbox_images = [cv2.resize(row_img[ry:ry+rh, rx:rx+rw].copy(), (16,16)) for (rx, ry, rw, rh) in row_rects]\n",
    "    return row_checkbox_images\n",
    "\n",
    "def get_checkbox_values(row_checkbox_images, field_labels):\n",
    "    for [label, check] in zip(field_labels, row_checkbox_images):\n",
    "        print(label)\n",
    "        show(check, figsize=(1, 1))\n",
    "\n",
    "    row_blue = [only_blue_pixels(row_rect) for row_rect in row_checkbox_images]\n",
    "    row_check_gray = [255 - cv2.cvtColor(row_rect, cv2.COLOR_BGR2GRAY) for row_rect in row_blue]\n",
    "\n",
    "    row_is_checked = [np.sum(rect) > 500 for rect in row_check_gray]\n",
    "\n",
    "    return row_is_checked\n",
    "\n",
    "def get_row_choices(row, field_labels):\n",
    "    # step 1: get images of checkboxes for line\n",
    "    row_checkbox_images = get_checkbox_images(row, field_labels)\n",
    "\n",
    "    # step 2: find out if checkboxes are checked or not\n",
    "    return get_checkbox_values(row_checkbox_images, field_labels)\n",
    "\n",
    "get_row_choices(sections.loc[3,8], [\"Neuerwerb\", \"Grabstein / Info Steinmetz\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d4ab16fab9499e7",
   "metadata": {},
   "source": [
    "choices = pd.read_csv(\"data/choices.csv\")\n",
    "\n",
    "test = pd.DataFrame(choices.groupby([\"section_index\", \"field_index\"])['label'].apply(list))\n",
    "\n",
    "test[\"choices\"] = test.apply(lambda x: get_row_choices(sections.loc[x.name], x.label), axis=1)\n",
    "\n",
    "sections[\"choice_labels\"] = test[\"label\"]\n",
    "sections[\"choices\"] = test[\"choices\"]\n",
    "sections\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
